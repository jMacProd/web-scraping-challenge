{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission to Mars\n",
    "* Developing the webscraping code to be used in a Flask web app.\n",
    "* Using Jupyter Notebook, BeautifulSoup, Pandas, and Requests/Splinter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#enables to read and search html\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#allows Python to reach out to the internet \n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 88.0.4324\n",
      "[WDM] - Get LATEST driver version for 88.0.4324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [/Users/J/.wdm/drivers/chromedriver/mac64/88.0.4324.96/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "#splinter actications\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA Mars News\n",
    "* Scraping the [NASA Mars News Site](https://mars.nasa.gov/news/) to collect the latest News Title and Paragraph Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tell the splinter browser to go to this website\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object; parse with 'html.parser' - using the splinter browser\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect the image container\n",
    "\n",
    "results = soup.find('li', class_='slide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the text to variables that you can reference later\n",
    "\n",
    "news_title = []\n",
    "paragraph_text = []\n",
    "\n",
    "   \n",
    "# Retrieve the news item title\n",
    "title = results.find('div', class_='content_title')\n",
    "    \n",
    "# Access the titles text content\n",
    "title_text = title.a.text\n",
    "    \n",
    "# Retrieve the Paragraph Text\n",
    "article_teaser = results.find('div', class_='article_teaser_body')\n",
    "    \n",
    "# Access the paragraphs's text content\n",
    "article_teaser_text = article_teaser.text\n",
    "    \n",
    "#Append the lists\n",
    "news_title.append(title_text)\n",
    "paragraph_text.append(article_teaser_text)\n",
    "    \n",
    "    \n",
    "#print(\"\")\n",
    "#print(title_text)\n",
    "#print(\"--\")\n",
    "#print(article_teaser_text)\n",
    "#print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check news_title\n",
    "#news_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check paragraph text\n",
    "#paragraph_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Close the browser after scraping\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPL Mars Space Images - Featured Image\n",
    "*  Using splinter to navigate the site and collect the image url for the current Featured Mars Image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set new browser\n",
    "browserfi = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "#Tell the splinter browser to go to this website\n",
    "\n",
    "urlfi = 'https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/'\n",
    "browserfi.visit(urlfi + 'index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object; parse with 'html.parser' - using the splinter browser\n",
    "htmlfi = browserfi.html\n",
    "soupfi = bs(htmlfi, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soupfi.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect the image container\n",
    "\n",
    "resultsfi = soupfi.find('div', class_='floating_text_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resultsfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the url string to a variable called `featured_image_url`.\n",
    "\n",
    "link = resultsfi.a['href']\n",
    "featured_image_url = urlfi + link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check image url\n",
    "#featured_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserfi.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Facts Table\n",
    "* Using Pandas to scraping the table containing facts about the planet including Diameter, Mass, etc from the [Mars Facts webpage](https://space-facts.com/mars/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visit the Mars Facts webpage [here](https://space-facts.com/mars/)\n",
    "urlmf = 'https://space-facts.com/mars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "    # Use Panda's `read_html` to parse the url\n",
    "\n",
    "tables = pd.read_html(urlmf)\n",
    "#tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_fact = tables[0]\n",
    "#mars_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index\n",
    "mars_fact.set_index(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check table\n",
    "#mars_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove index label\n",
    "mars_fact.index.names = ['']\n",
    "#mars_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset columns\n",
    "mars_fact.columns = ['Mars']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check mars_fact\n",
    "#mars_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Pandas to convert the data to a HTML table string.\n",
    "mars_fact_html = mars_fact.to_html()\n",
    "#mars_fact_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You may have to strip unwanted newlines to clean up the table.\n",
    "mars_fact_html = mars_fact_html.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mars_fact_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Hemispheres\n",
    "Visiting the [USGS Astrogeology site](https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) to obtain high resolution images for each of Mar's hemispheres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "#set new browser\n",
    "browsermh = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "#Tell the splinter browser to go to this website\n",
    "\n",
    "urlmh = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browsermh.visit(urlmh)     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define main url\n",
    "main_page = \"https://astrogeology.usgs.gov\"\n",
    "#print(main_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list called hemisphere_image_urls\n",
    "\n",
    "hemheader = []\n",
    "hemisphere_image_urls = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through returned results\n",
    "for x in range(0, 4):\n",
    "    \n",
    "    sleep(1)\n",
    "    \n",
    "    # converting splinter browser to html.\n",
    "    htmlmh = browsermh.html\n",
    "    \n",
    "    #Parse it to Beautiful Soup - makes the html searchable by tags\n",
    "    soupmh = bs(htmlmh, 'html.parser')\n",
    "    \n",
    "    \n",
    "    #Find the link container\n",
    "    resultsmh = soupmh.find_all('div', class_='description')\n",
    "    #print(resultsmh[x])\n",
    "    \n",
    "    \n",
    "    #Retrieve the link container\n",
    "    linkcontmh = resultsmh[x].a['href']\n",
    "    #print(linkcontmh)\n",
    "    \n",
    "    #Retrieve header\n",
    "    header = resultsmh[x].a.text\n",
    "    #print(header)\n",
    "    #print(\"--\")\n",
    "    \n",
    "    \n",
    "    #clicks on x link to go to next page\n",
    "    browsermh.visit(main_page+linkcontmh)\n",
    "    \n",
    "    sleep(1)\n",
    "    #print(browsermh.url)\n",
    "    \n",
    "    #Create a new html splinter browser and parse into beautifulsoup\n",
    "    htmlnp = browsermh.html\n",
    "    soupnp = bs(htmlnp, 'html.parser')\n",
    "    \n",
    "    #Collect np link container\n",
    "    resultsnp = soupnp.find('div', class_='wide-image-wrapper')\n",
    "    #print(resultsnp)\n",
    "    \n",
    "    #Find the second img - which is the fullsized inmage that is not he download version\n",
    "    find_images = resultsnp.find_all('img')[1]\n",
    "    \n",
    "    \n",
    "    #collect the src link\n",
    "    litag = find_images['src']\n",
    "    \n",
    "\n",
    "    #Create complete link to image          \n",
    "    imgnp_url = main_page+litag\n",
    "    #print(imgnp_url)\n",
    "          \n",
    "    #print(\"--------\")\n",
    "          \n",
    " \n",
    "   \n",
    "    #Append the lists\n",
    "    hemheader.append(header)\n",
    "    hemisphere_image_urls.append(imgnp_url)\n",
    "    \n",
    "       \n",
    "        \n",
    "    #hemisphere_image_urls.append(link_dict)\n",
    "    \n",
    "    \n",
    "    #Go back to main page\n",
    "    browsermh.back()\n",
    "\n",
    "browsermh.quit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check lists\n",
    "#hemheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module used to connect Python with MongoDb\n",
    "import pymongo\n",
    "# The default port used by MongoDB is 27017\n",
    "# https://docs.mongodb.com/manual/reference/default-mongodb-port/\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "# Define the 'classDB' database in Mongo\n",
    "db = client.marsDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "* convert jupyter notebook to python - executed in Mac OS Terminal with following command:\n",
    "    * jupyter nbconvert --to script mission_to_mars.ipynb\n",
    "    * Source: https://nbconvert.readthedocs.io/en/latest/usage.html\n",
    "* Renamed python file to scrape_mars.py\n",
    "* Use the scraping code to define a function in the scrape_mars.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
